# 2.2 数据操作

> "tensor"这个单词一般可译作“张量”，张量可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。

## 2.2.1 创建`Tensor`

- 存储向量与标量

```python
torch.tensor(1) # 标量
torch.tensor([1]) # 1*1 的向量
```

- 创建一个5x3的未初始化的`Tensor`：

``` python
import torch
x = torch.empty(5, 3)
```
- 创建一个5x3的随机初始化的`Tensor`:

``` python
x = torch.rand(5, 3)
```
- 创建一个5x3的long型全0的`Tensor`:

``` python
x = torch.zeros(5, 3, dtype=torch.long)
```
- 直接根据数据创建:

``` python
x = torch.tensor([5.5, 3])
```
- 通过现有的`Tensor`来创建，此方法会默认重用输入`Tensor`的一些属性，例如数据类型，除非自定义数据类型。

```python
x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device

x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型
```

- 通过`shape`或者`size()`来获取`Tensor`的形状:

``` python
print(x.size())
print(x.shape)
```
> 注意：返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。

还有很多函数可以创建`Tensor`，去翻翻官方API就知道了，下表给了一些常用的作参考。

|函数|功能|
|:---:|:---:|
|Tensor(*sizes)|基础构造函数|
|tensor(data,)|类似np.array的构造函数|
|ones(*sizes)|全1Tensor|
|zeros(*sizes)|全0Tensor|
|eye(*sizes)|对角线为1，其他为0|
|arange(s,e,step)|从s到e，步长为step|
|linspace(s,e,steps)|从s到e，均匀切分成steps份|
|rand/randn(*sizes)|均匀/标准分布|
|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|
|randperm(m)|随机排列|

这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。

## 2.2.2 操作
### 算术操作

- 加

  - 加号`+`

  - torch.add(x, y)

  - 指定输出

    ```python
    result = torch.empty(5, 3)
    torch.add(x, y, out=result)
    print(result)
    ```

  * **inplace**

      ``` python
      # adds x to y
      y.add_(x)
      print(y)
      ```
      > **注：PyTorch操作inplace版本都有后缀`_`, 例如`x.copy_(y), x.t_()`**

### 索引
`索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改 `

``` python
y = x[0, :]
y += 1
print(y)
print(x[0, :]) # 源tensor也被改了
```
除了常用的索引选择数据之外，PyTorch还提供了一些高级的选择函数:

|函数|	功能|
|:---:|:---:|
|index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列|
|masked_select(input, mask)|例子如上，a[a>0]，使用ByteTensor进行选取|
|nonzero(input)|	非0元素的下标|
|gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样|

这里不详细介绍，用到了再查官方文档。
### 改变形状

- view()共享data,更改新tensor，源tensor会更改
- `reshape()`可以改变形状，不能保证返回的是其拷贝。用`clone`创造一个副本然后再使用`view`。[参考此处](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch)

``` python
x_cp = x.clone().view(15)
```
> 使用`clone`还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源`Tensor`。

- `item()`, 将一个标量`Tensor`转换成一个Python number：

``` python
x = torch.randn(1)
print(x)
print(x.item())
# tensor([2.3466])
# 2.3466382026672363
```
### 线性代数
| 函数	|功能|
|:---:|:---:|
|trace|	对角线元素之和(矩阵的迹)|
|diag|	对角线元素|
|triu/tril	|矩阵的上三角/下三角，可指定偏移量|
|mm/bmm	|矩阵乘法，batch的矩阵乘法|
|addmm/addbmm/addmv/addr/baddbmm..|	矩阵运算|
|t|转置|
|dot/cross|	内积/外积|
|inverse	|求逆矩阵|
|svd	|奇异值分解|

## 2.2.3 广播机制

不同维度的相加会触发广播机制

## 2.2.4 运算的内存开销
- 索引操作不会开辟新内存，`y = x + y`这样的运算是会新开内存的，然后将`y`指向新内存。

``` python
x = torch.tensor([1, 2])
y = torch.tensor([3, 4])
id_before = id(y)
y = y + x
print(id(y) == id_before) # False 
```

- 如果想指定结果到原来的`y`的内存，把`x + y`的结果通过`[:]`写进`y`对应的内存中。

``` python
x = torch.tensor([1, 2])
y = torch.tensor([3, 4])
id_before = id(y)
y[:] = y + x
print(id(y) == id_before) # True
```
- `out`参数或者自加运算符`+=`(也即`add_()`)达到上述效果，例如`torch.add(x, y, out=y)`和`y += x`(`y.add_(x)`)。

``` python
x = torch.tensor([1, 2])
y = torch.tensor([3, 4])
id_before = id(y)
torch.add(x, y, out=y) # y += x, y.add_(x)
print(id(y) == id_before) # True
```

> 注：虽然`view`返回的`Tensor`与源`Tensor`是共享`data`的，但是依然是一个新的`Tensor`（因为`Tensor`除了包含`data`外还有一些其他属性），二者id（内存地址）并不一致。

## 2.2.5 `Tensor`与NumPy转换

- tensor转numpy
  - a.numpy()
- numpy转tensor
  - torch.from_numpy(a)
  - torch.tensor():会进行数据拷贝（就会消耗更多的时间和空间）,不共享内存

- 所有在CPU上的`Tensor`（除了`CharTensor`）都支持与NumPy数组相互转换。

## 2.2.6 `Tensor` on GPU
- `to()`可以将`Tensor`在CPU和GPU（需要硬件支持）之间相互移动。

``` python
# 以下代码只有在PyTorch GPU版本上才会执行
if torch.cuda.is_available():
    device = torch.device("cuda")          # GPU
    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor
    x = x.to(device)                       # 等价于 .to("cuda")
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # to()还可以同时更改数据类型
```

